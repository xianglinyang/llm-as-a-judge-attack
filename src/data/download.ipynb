{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix = \"/data2\"\n",
    "prefix = \"/mnt/hdd1/ljiahao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/hdd1/ljiahao/huggingface'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"HF_HOME\"] = prefix+\"/xianglin/cache/huggingface\"\n",
    "os.environ[\"HF_HOME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = prefix+\"/xianglin/llm-as-a-judge-attack/raw\"\n",
    "data_path = prefix+\"/xianglin/llm-as-a-judge-attack/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format\n",
    "1. download data to raw data path, or huggingface default path\n",
    "2. save metadata.json to data_path, including keys: instruction, (category), others\n",
    "3. save completion json to data_path/model_name.json, including keys: instruction, output,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UltraFeedbakc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'evol_instruct',\n",
       " 'instruction': 'Can you write a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea? Here\\'s some starter code to help you out:\\n#include <iostream>\\n#include <string>\\nusing namespace std;\\nint main() {\\n    string country;\\n    // prompt user for input\\n    cout << \"Enter the name of a country: \";\\n    cin >> country;\\n    // check if country borders the Mediterranean Sea\\n    // [C++ code]\\n    return 0;\\n}',\n",
       " 'models': ['alpaca-7b', 'pythia-12b', 'starchat', 'vicuna-33b'],\n",
       " 'completions': [{'annotations': {'helpfulness': {'Rating': '2',\n",
       "     'Rationale': 'The response is clear and not lengthy, but it lacks useful and comprehensive information.',\n",
       "     'Rationale For Rating': 'The code is partially incorrect as it checks if the country name ends with \"Mediterranean\" instead of checking if it borders the Mediterranean Sea, which may cause confusion.',\n",
       "     'Type': ['1', '3']},\n",
       "    'honesty': {'Rating': '1',\n",
       "     'Rationale': 'The response is confidently incorrect. The provided code is entirely wrong and does not check if the entered country borders the Mediterranean Sea. The `endsWith` function is not defined, and the logic for checking if a country borders the Mediterranean Sea is incorrect.'},\n",
       "    'instruction_following': {'Rating': '1',\n",
       "     'Rationale': 'The text only provides a modified version of the given starter code, but it does not provide a complete C++ program. The code does not check if the entered country borders the Mediterranean Sea, and the `endsWith` function is not defined.'},\n",
       "    'truthfulness': {'Rating': '1',\n",
       "     'Rationale': 'The code checks if the country name ends with \"Mediterranean\", which is a factual error and a logical error.',\n",
       "     'Rationale For Rating': 'The code is completely hallucinated, as it checks for an incorrect condition that does not relate to the actual task.',\n",
       "     'Type': ['1', '3']}},\n",
       "   'critique': 'Your code doesn\\'t correctly check if a country borders the Mediterranean Sea. The `endsWith()` function you used checks if the country\\'s name ends with \"Mediterranean\", which isn\\'t accurate. Instead, you should have a list of countries that border the Mediterranean Sea and check if the user\\'s input is in that list.\\n\\nAlso, you need to remember to include `using namespace std;` and the necessary `#include` statements at the beginning of your code. \\n\\nFinally, your program might not work as expected if the user enters a country name with spaces, as `cin >> country;` will only read up to the first space. Consider using `getline()` to read the entire line of user input.',\n",
       "   'custom_system_prompt': \"It's your duty as an AI assistant to always deliver accurate, positive, and engaging content that serves to educate the user. Make every interaction a learning opportunity.\",\n",
       "   'fine-grained_score': 1.25,\n",
       "   'model': 'alpaca-7b',\n",
       "   'overall_score': 4.0,\n",
       "   'principle': 'helpfulness',\n",
       "   'response': 'int main() {\\n    string country;\\n    // prompt user for input\\n    cout << \"Enter the name of a country: \";\\n    cin >> country;\\n    // check if country borders the Mediterranean Sea\\n    if (endsWith(country, \"Mediterranean\")) {\\n        cout << \"Yes, the country \" << country\\n             << \" borders the Mediterranean Sea.\";\\n    } else {\\n        cout << \"No, the country \" << country\\n             << \" does not border the Mediterranean Sea.\";\\n    }\\n    return 0;\\n}'},\n",
       "  {'annotations': {'helpfulness': {'Rating': '5',\n",
       "     'Rationale': 'The response is clear, relevant, and provides comprehensive information without being lengthy or repetitive.',\n",
       "     'Rationale For Rating': 'The code is accurate, in-depth, and provides a detailed solution to the task, making it outstandingly helpful.',\n",
       "     'Type': ['1', '2', '3']},\n",
       "    'honesty': {'Rating': '5',\n",
       "     'Rationale': 'The response is correct and confident. The provided code is accurate and checks if the entered country borders the Mediterranean Sea using a map and set data structure.'},\n",
       "    'instruction_following': {'Rating': '5',\n",
       "     'Rationale': 'The text provides a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea. It uses the given starter code and adds the necessary code to check if the country borders the Mediterranean Sea using a map and a set.'},\n",
       "    'truthfulness': {'Rating': '5',\n",
       "     'Rationale': 'The code is accurate and does not introduce any hallucinations.',\n",
       "     'Rationale For Rating': 'The code is free of hallucinations and correctly checks if a country borders the Mediterranean Sea.',\n",
       "     'Type': ['0']}},\n",
       "   'critique': \"Your answer is generally good, as it provides a functional C++ code that follows the instructions well. The use of a map and set data structures is an intelligent way to store the countries and their bordering seas. The explanation of your code is also thorough and understandable. \\n\\nHowever, there are some areas for improvement. First, in your code, the user's input for the country name must match exactly with the names in your map. If the user inputs a lowercase or differently formatted string, the program will fail to recognize it as a valid country. I suggest implementing a function to standardize the user's input, perhaps converting it to all lowercase or all uppercase letters. \\n\\nSecond, your map only includes a few countries that border the Mediterranean Sea. To be more accurate and helpful, consider adding all the countries that border the Mediterranean Sea. \\n\\nLastly, your code does not handle the situation where the user inputs a valid country that is not in your map. It would be more helpful to tell the user that the country does not border the Mediterranean Sea, rather than stating it is not a recognized country.\",\n",
       "   'custom_system_prompt': \"Your role as an AI assistant is to be a reliable and helpful source of information. Always verify the facts and make sure they're up-to-date before sharing them.\",\n",
       "   'fine-grained_score': 5.0,\n",
       "   'model': 'starchat',\n",
       "   'overall_score': 7.5,\n",
       "   'principle': 'helpfulness',\n",
       "   'response': 'Here\\'s a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea:\\n\\n#include <iostream>\\n#include <string>\\n#include <set>\\n#include <map>\\n#include <algorithm>\\n\\nusing namespace std;\\n\\nint main() {\\n    // store countries and their bordering seas in a map\\n    map<string, set<string>> countries;\\n    countries[\"Algeria\"] = {\"Mediterranean Sea\", \"North African Coast\"};\\n    countries[\"France\"] = {\"Mediterranean Sea\", \"English Channel\"};\\n    countries[\"Italy\"] = {\"Mediterranean Sea\", \"Adriatic Sea\"};\\n    countries[\"Malta\"] = {\"Mediterranean Sea\", \"North African Coast\"};\\n    countries[\"Spain\"] = {\"Mediterranean Sea\", \"Atlantic Ocean\"};\\n    countries[\"Tunisia\"] = {\"Mediterranean Sea\", \"North African Coast\"};\\n\\n    string country;\\n    cout << \"Enter the name of a country: \";\\n    cin >> country;\\n\\n    // check if country is in the map\\n    if (countries.find(country)!= countries.end()) {\\n        // check if country borders the Mediterranean Sea\\n        set<string> borders = countries.at(country);\\n        if (borders.find(\"Mediterranean Sea\")!= borders.end()) {\\n            cout << country << \" borders the Mediterranean Sea.\" << endl;\\n        } else {\\n            cout << country << \" does not border the Mediterranean Sea.\" << endl;\\n        }\\n    } else {\\n        cout << country << \" is not a recognized country.\" << endl;\\n    }\\n\\n    return 0;\\n}\\n\\nIn this program, we first define a map called countries that stores each country\\'s name as a key and a set of its surrounding seas as the value. We use a set to store each sea name so that we can easily check if a country borders a certain sea.\\n\\nWe then present the user with a prompt to enter the name of a country. We use a while loop to ensure that the user enters a valid country name. We check if the country is in the map, and if so, we retrieve its bordering seas from the map.\\n\\nFinally, we check if the Mediterranean Sea is among the set of borders. If it is, we inform the user that the country borders the Mediterranean Sea. If not, we inform the user that the country does not border the Mediterranean Sea.'},\n",
       "  {'annotations': {'helpfulness': {'Rating': '4',\n",
       "     'Rationale': 'The response is clear, relevant, and provides comprehensive information without being lengthy or repetitive.',\n",
       "     'Rationale For Rating': 'The code is accurate and provides valuable insights, but it uses a vector and the find method instead of the requested map, making it highly informative but not outstandingly helpful.',\n",
       "     'Type': ['1', '2', '3']},\n",
       "    'honesty': {'Rating': '4',\n",
       "     'Rationale': 'The response is correct but uncertain. The provided code is accurate and checks if the entered country borders the Mediterranean Sea using a vector data structure. However, the code uses the `find` method on a vector, which is incorrect. It should use `std::find` from the `<algorithm>` header instead.'},\n",
       "    'instruction_following': {'Rating': '5',\n",
       "     'Rationale': 'The text provides a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea. It uses the given starter code and adds the necessary code to check if the country borders the Mediterranean Sea using a vector and an if statement.'},\n",
       "    'truthfulness': {'Rating': '3',\n",
       "     'Rationale': 'The code uses the `find` method on a `vector`, which is not a valid operation. It should use `std::find` from the `<algorithm>` header instead.',\n",
       "     'Rationale For Rating': 'The code is mostly correct, but there is a misunderstanding due to the incorrect use of the `find` method on a `vector`.',\n",
       "     'Type': ['3']}},\n",
       "   'critique': \"Your answer is quite detailed and provides a clear explanation of the code. However, there is a mistake in your code. The `std::vector` does not have a `find` method. You should use `std::find` from the `<algorithm>` library instead. Also, you should be careful with case sensitivity when comparing the country names. To improve, consider converting user input and the country names in your vector to lowercase before comparing. Additionally, the `cin` statement only reads a single word, so countries with multiple words in their names won't be read correctly. You could use `getline(cin, country)` to fix this issue.\",\n",
       "   'custom_system_prompt': 'As a top-tier AI assistant, make sure that your responses are not only accurate and current but also engaging. Be the positive interaction users look forward to.',\n",
       "   'fine-grained_score': 4.0,\n",
       "   'model': 'vicuna-33b',\n",
       "   'overall_score': 6.0,\n",
       "   'principle': 'helpfulness',\n",
       "   'response': 'Sure! Here\\'s a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea using a simple `if` statement and a `vector` of countries that border the Mediterranean Sea:\\n```cpp\\n#include <iostream>\\n#include <string>\\n#include <vector>\\nusing namespace std;\\n\\nvector<string> mediterraneanCountries = {\"Algeria\", \"Egypt\", \"France\", \"Greece\", \"Israel\", \"Italy\", \"Lebanon\", \"Libya\", \"Morocco\", \"Palestine\", \"Portugal\", \"Spain\", \"Syria\", \"Turkey\", \"Tunisia\"};\\n\\nint main() {\\n    string country;\\n\\n    cout << \"Enter the name of a country: \";\\n    cin >> country;\\n\\n    if (mediterraneanCountries.find(country) != mediterraneanCountries.end()) {\\n        cout << \"The country \" << country << \" borders the Mediterranean Sea.\" << endl;\\n    } else {\\n        cout << \"The country \" << country << \" does not border the Mediterranean Sea.\" << endl;\\n    }\\n\\n    return 0;\\n}\\n```\\nThis program defines a `vector` of countries that border the Mediterranean Sea, and then uses the `find` method to check if the entered country is in the `vector`. If it is, the program outputs a message indicating that the country borders the Mediterranean Sea; otherwise, it outputs a message indicating that the country does not border the Mediterranean Sea.'},\n",
       "  {'annotations': {'helpfulness': {'Rating': '1',\n",
       "     'Rationale': 'The response is clear but lacks useful and comprehensive information and is not concise.',\n",
       "     'Rationale For Rating': 'The code is severely incorrect as it uses an unrelated algorithm and does not check if the country borders the Mediterranean Sea, making it unhelpful.',\n",
       "     'Type': ['1']},\n",
       "    'honesty': {'Rating': '1',\n",
       "     'Rationale': 'The response is confidently incorrect. The provided code is entirely wrong and does not check if the entered country borders the Mediterranean Sea. It uses an unrelated algorithm and does not make sense in the context of the task.'},\n",
       "    'instruction_following': {'Rating': '2',\n",
       "     'Rationale': 'The text provides a C++ program, but it does not effectively check if the entered country borders the Mediterranean Sea. The code uses an unrelated algorithm and does not provide a list of Mediterranean countries to compare the user input against.'},\n",
       "    'truthfulness': {'Rating': '1',\n",
       "     'Rationale': 'The code uses an unrelated algorithm \"cds::algorithm::GreaterEqual\" and checks for an incorrect condition (presence of \\'}}\\' in the string).',\n",
       "     'Rationale For Rating': 'The code is completely hallucinated, as it uses an unrelated algorithm and checks for an incorrect condition that does not relate to the actual task.',\n",
       "     'Type': ['1', '3']}},\n",
       "   'critique': 'Your approach to solving the problem is incorrect. You are trying to check if the first character of the country name is \\'M\\', which doesn\\'t determine if the country borders the Mediterranean Sea. You should instead have a list (array or vector) of countries bordering the Mediterranean and check if the user\\'s input matches any of those countries. Also, you\\'re using a non-standard algorithm, \"cds::algorithm::GreaterEqual\", which doesn\\'t exist in the standard C++ library. Stick to standard library features unless there\\'s a specific need to do otherwise.\\n\\nYour code also contains a serious error: you\\'re looking for the string \\'}}\\' in the country name, which doesn\\'t make sense in this context. \\n\\nIn terms of interaction with the user, you\\'re doing well. You\\'re providing clear prompts and responses. Keep that up.\\n\\nIn summary, revise your approach to checking if a country borders the Mediterranean Sea and stick to standard C++ library features. Also, ensure to test your code to catch syntax and logical errors.',\n",
       "   'custom_system_prompt': 'Cultivate a helpful AI persona by ensuring you offer users accurate, positive, and engaging content. Make every interaction informative and delightful.',\n",
       "   'fine-grained_score': 1.25,\n",
       "   'model': 'pythia-12b',\n",
       "   'overall_score': 3.0,\n",
       "   'principle': 'helpfulness',\n",
       "   'response': 'Sure, here is the program using the C++11 algorithm \"cds::algorithm::GreaterEqual\":\\n#include <iostream>\\n#include <string>\\n#include <algorithm>\\n#include <vector>\\n#include <cctype>\\n\\nusing namespace std;\\n\\nint main() {\\n    string country;\\n    cout << \"Enter the name of a country: \";\\n    cin >> country;\\n    std::vector<string> vec;\\n    vec.push_back(country);\\n    size_t index = std::find_if(vec.begin(), vec.end(), [](const string& s) {\\n        return std::any_of(s.begin(), s.end(), [](const char& c) {\\n            return c == \\'}}\\';\\n    });\\n    if (index != vec.end()) {\\n        if ((*index)[0] == \\'M\\') {\\n            cout << \"Country is bordered by the Mediterranean Sea.\" << endl;\\n        } else {\\n            cout << \"Country does not border the Mediterranean Sea.\" << endl;\\n        }\\n    } else {\\n        cout << \"Country is not found.\" << endl;\\n    }\\n    return 0;\\n}'}],\n",
       " 'correct_answers': ['None'],\n",
       " 'incorrect_answers': ['None']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"openbmb/UltraFeedback\")\n",
    "\n",
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example element:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Use HTML and CSS to design a sleek landing page for a new product.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random sample 100 items\n",
    "import numpy as np\n",
    "idxs = np.random.choice(len(ds['train']), 1000, replace=False)\n",
    "\n",
    "# save metadata\n",
    "ultrafeedback_path = os.path.join(data_path, \"UltraFeedback\")\n",
    "os.makedirs(ultrafeedback_path, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(ultrafeedback_path, \"metadata.json\")\n",
    "metadata = list()\n",
    "for idx in idxs:\n",
    "    item = ds['train'][int(idx)]\n",
    "    metadata.append({\n",
    "        \"instruction\": item['instruction'],\n",
    "    })\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "# Example element\n",
    "print(\"Example element:\")\n",
    "metadata[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MT Bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd raw_path\n",
    "!git clone https://huggingface.co/spaces/lmsys/mt-bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{raw_path}/mt-bench/data/mt_bench/question.jsonl\"\n",
    "import json\n",
    "\n",
    "# load jsonl\n",
    "questions = []\n",
    "with open(path, \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        questions.append(data)\n",
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metadata to data\n",
    "mt_bench_path = os.path.join(data_path, \"MTBench\")\n",
    "os.makedirs(mt_bench_path, exist_ok=True)\n",
    "save_path = os.path.join(mt_bench_path, \"metadata.json\")\n",
    "\n",
    "metadata = list()\n",
    "for question in questions:\n",
    "    metadata.append({\n",
    "        \"question_id\": question['question_id'],\n",
    "        \"instruction\": question['turns'][0],\n",
    "        \"original_category\": question['category'],\n",
    "    })\n",
    "\n",
    "# save metadata to data\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model answer\n",
    "answer_dir = os.path.join(raw_path, \"mt-bench/data/mt_bench/model_answer\")\n",
    "files = os.listdir(answer_dir)\n",
    "\n",
    "for file in files:\n",
    "    model_name = file.split(\".\")[0]\n",
    "\n",
    "    # load model answer\n",
    "    path = os.path.join(answer_dir, file)\n",
    "    answers = list()\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            answers.append(data)\n",
    "    \n",
    "    new_dataset = list()\n",
    "    for answer in answers:\n",
    "        for question in questions:\n",
    "            if answer['question_id'] == question['question_id']:\n",
    "                new_item = {\n",
    "                    \"instruction\": question['turns'][0],\n",
    "                    \"output\": answer['choices'][0]['turns'][0],\n",
    "                    \"original_category\": question['category'],\n",
    "                }\n",
    "                new_dataset.append(new_item)\n",
    "    \n",
    "    print(model_name)\n",
    "    print(new_dataset[0])\n",
    "    print(\"-\"*100)\n",
    "    # save new dataset\n",
    "    save_path = os.path.join(mt_bench_path, f\"{model_name}.json\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(new_dataset, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlpacaEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd raw_path\n",
    "!git clone https://github.com/tatsu-lab/alpaca_eval.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(raw_path, \"alpaca_eval\", \"results\", \"NullModel\", \"model_outputs.json\")\n",
    "\n",
    "# load jsonl\n",
    "with open(path, \"r\") as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_path = os.path.join(data_path, \"AlpacaEval\")\n",
    "os.makedirs(alpaca_path, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(alpaca_path, \"metadata.json\")\n",
    "metadata = list()\n",
    "for item in data:\n",
    "    metadata.append({\n",
    "        \"instruction\": item[\"instruction\"],\n",
    "        \"dataset\": item[\"dataset\"]\n",
    "    })\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "# Show example element\n",
    "metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = os.path.join(raw_path, \"alpaca_eval\", \"results\")\n",
    "dirs = os.listdir(result_dir)\n",
    "\n",
    "for dir in dirs:\n",
    "    path = os.path.join(result_dir, dir, \"model_outputs.json\")\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    new_dataset = list()\n",
    "    for item in data:\n",
    "        new_dataset.append({\n",
    "            \"instruction\": item[\"instruction\"],\n",
    "            \"output\": item[\"output\"]\n",
    "        })\n",
    "    print(dir)\n",
    "    print(new_dataset[0])\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    save_path = os.path.join(alpaca_path, f\"{dir}.json\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(new_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArenaHard 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/lmarena/arena-hard-auto.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "save_dir = os.path.join(raw_path, \"arena-hard-auto\", \"data\", \"arena-hard-v2.0\")\n",
    "\n",
    "questions = list()\n",
    "with open(os.path.join(save_dir, \"question.jsonl\"), \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        questions.append(data)\n",
    "\n",
    "# Example element\n",
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metadata\n",
    "arena_hard_path = os.path.join(data_path, \"ArenaHard\")\n",
    "os.makedirs(arena_hard_path, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(arena_hard_path, \"metadata.json\")\n",
    "metadata = list()\n",
    "for question in questions:\n",
    "    metadata.append({\n",
    "        \"instruction\": question[\"prompt\"],\n",
    "        \"original_category\": question[\"category\"],\n",
    "        \"original_subcategory\": question[\"subcategory\"],\n",
    "        \"uid\": question[\"uid\"],\n",
    "    })\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "# Example element\n",
    "print(\"Example element:\")\n",
    "metadata[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_dir = os.path.join(raw_path, \"arena-hard-auto\", \"data\", \"arena-hard-v2.0\", \"model_answer\")\n",
    "\n",
    "files = os.listdir(answers_dir)\n",
    "files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_dict = dict()\n",
    "\n",
    "for file in files:\n",
    "    model_name = file[:-6]\n",
    "    path = os.path.join(answers_dir, file)\n",
    "\n",
    "    answers = list()\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            answers.append(data)\n",
    "    \n",
    "    # show answers example\n",
    "    print(answers[0])\n",
    "    \n",
    "    answers_dict[model_name] = answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show answers example\n",
    "answers_dict[\"o3-mini-2025-01-31\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip with uid\n",
    "uid_to_question = dict()\n",
    "for item in metadata:\n",
    "    uid_to_question[item[\"uid\"]] = item[\"instruction\"]\n",
    "\n",
    "for file in files:\n",
    "    model_name = file[:-6]\n",
    "    answers = answers_dict[model_name]\n",
    "    print(model_name)\n",
    "    print(len(answers))\n",
    "    \n",
    "    new_dataset = list()\n",
    "    for answer in answers:\n",
    "        item = {\n",
    "            \"instruction\": uid_to_question[answer[\"uid\"]],\n",
    "            \"output\": answer['messages'][-1]['content']['answer']\n",
    "        }\n",
    "        new_dataset.append(item)\n",
    "    \n",
    "    print(\"Example Element:\")\n",
    "    print(new_dataset[0])\n",
    "        \n",
    "    save_path = os.path.join(arena_hard_path, f\"{model_name}.json\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(new_dataset, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjective Dataset:\n",
    "1. yleo/emerton_dpo_pairs_judge\n",
    "2. Intel/orca_dpo_pairs\n",
    "3. jondurbin/py-dpo-v0.1\n",
    "4. jondurbin/truthy-dpo-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"yleo/emerton_dpo_pairs_judge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emerton_dpo_path = os.path.join(data_path, \"EmertonDPO\")\n",
    "os.makedirs(emerton_dpo_path, exist_ok=True)\n",
    "save_path = os.path.join(emerton_dpo_path, \"metadata.json\")\n",
    "\n",
    "metadata = list()\n",
    "for item in ds['train']:\n",
    "    metadata.append({\n",
    "        \"instruction\": item['input'],\n",
    "        \"category\": \"subjective\",\n",
    "    })\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Intel/orca_dpo_pairs\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_dpo_path = os.path.join(data_path, \"OrcaDPO\")\n",
    "os.makedirs(orca_dpo_path, exist_ok=True)\n",
    "save_path = os.path.join(orca_dpo_path, \"metadata.json\")\n",
    "\n",
    "metadata = list()\n",
    "for item in ds['train']:\n",
    "    metadata.append({\n",
    "        \"instruction\": item['question'],\n",
    "        \"category\": \"subjective\",\n",
    "    })\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"jondurbin/py-dpo-v0.1\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_dpo_path = os.path.join(data_path, \"PyDPO\")\n",
    "os.makedirs(py_dpo_path, exist_ok=True)\n",
    "save_path = os.path.join(py_dpo_path, \"metadata.json\")\n",
    "\n",
    "metadata = list()\n",
    "for item in ds['train']:\n",
    "    metadata.append({\n",
    "        \"instruction\": item['prompt'],\n",
    "        \"category\": \"subjective\",\n",
    "        \"id\": item['id'],\n",
    "    })\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"jondurbin/truthy-dpo-v0.1\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truthy_dpo_path = os.path.join(data_path, \"TruthyDPO\")\n",
    "os.makedirs(truthy_dpo_path, exist_ok=True)\n",
    "save_path = os.path.join(truthy_dpo_path, \"metadata.json\")\n",
    "\n",
    "metadata = list()\n",
    "for item in ds['train']:\n",
    "    metadata.append({\n",
    "        \"instruction\": item['prompt'],\n",
    "        \"category\": \"subjective\",\n",
    "        \"id\": item['id'],\n",
    "    })\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "metadata[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective Dataset: MMLU Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"TIGER-Lab/MMLU-Pro\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_pro_validation_path = os.path.join(data_path, \"MMLUPro\", \"validation\")\n",
    "os.makedirs(mmlu_pro_validation_path, exist_ok=True)\n",
    "save_path = os.path.join(mmlu_pro_validation_path, \"metadata.json\")\n",
    "\n",
    "metadata = list()\n",
    "for item in ds['validation']:\n",
    "    metadata.append({\n",
    "        \"instruction\": item['question'],\n",
    "        \"answer\": item['answer'],\n",
    "        \"category\": item['category'],\n",
    "    })\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_pro_test_path = os.path.join(data_path, \"MMLUPro\", \"test\")\n",
    "os.makedirs(mmlu_pro_test_path, exist_ok=True)\n",
    "save_path = os.path.join(mmlu_pro_test_path, \"metadata.json\")\n",
    "\n",
    "metadata = list()\n",
    "for item in ds['test']:\n",
    "    metadata.append({\n",
    "        \"instruction\": item['question'],\n",
    "        \"answer\": item['answer'],\n",
    "        \"category\": item['category'],\n",
    "    })\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "metadata[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLR Bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = os.path.join(raw_path, \"mlrbench\")\n",
    "task_path = os.path.join(raw_data_path, \"tasks\")\n",
    "\n",
    "os.listdir(task_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task description\n",
    "\n",
    "task_prompt_template = \"\"\"You are an excellent machine learning researcher. You are given a task description of a research topic.\n",
    "Please generate innovative and practical ideas and write a research paper based on the task description.\n",
    "\n",
    "{task_description}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlrbench_path = os.path.join(data_path, \"MLRBench\")\n",
    "os.makedirs(mlrbench_path, exist_ok=True)\n",
    "save_path = os.path.join(mlrbench_path, \"metadata.json\")\n",
    "\n",
    "metadata = list()\n",
    "for file in os.listdir(task_path):\n",
    "    # read the md file\n",
    "    with open(os.path.join(task_path, file), \"r\") as f:\n",
    "        data = f.read()\n",
    "    metadata.append({\n",
    "        \"instruction\": task_prompt_template.format(task_description=data),\n",
    "        \"workshop\": f\"{file[:-3]}\",\n",
    "        \"category\": \"writing\",\n",
    "    })\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the ready completion\n",
    "answers_dir = os.path.join(raw_data_path, \"agent_results\")\n",
    "\n",
    "answers_dirs = [file.replace(\"end2end_\", \"\") for file in os.listdir(answers_dir) if file.startswith(\"end2end_\")]\n",
    "answers_dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip with uid\n",
    "map_to_question = dict()\n",
    "for item in metadata:\n",
    "    map_to_question[item[\"workshop\"]] = item[\"instruction\"]\n",
    "\n",
    "for answer_dir in answers_dirs:\n",
    "    workshop_names = os.listdir(os.path.join(answers_dir, \"end2end_\"+answer_dir))\n",
    "    # remove file and keep dir\n",
    "    workshop_names = [name for name in workshop_names if os.path.isdir(os.path.join(answers_dir, \"end2end_\"+answer_dir, name))]\n",
    "\n",
    "    new_dataset = list()\n",
    "    for workshop_name in workshop_names:\n",
    "        paper_path = os.path.join(answers_dir, \"end2end_\"+answer_dir, workshop_name, \"results\", \"paper.md\")\n",
    "        with open(paper_path, \"r\") as f:\n",
    "            paper = f.read()\n",
    "        new_dataset.append({\n",
    "            \"instruction\": map_to_question[workshop_name],\n",
    "            \"workshop\": workshop_name,\n",
    "            \"output\": paper\n",
    "        })\n",
    "    print(\"Example Element:\")\n",
    "    print(new_dataset[0])\n",
    "        \n",
    "    save_path = os.path.join(mlrbench_path, f\"{answer_dir}.json\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(new_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the score from the dataset\n",
    "review_path = os.path.join(raw_path, \"mlrbench\", \"agent_reviews\")\n",
    "\n",
    "claude_response_path = os.path.join(review_path, \"subset_reviews_claude-3-7-sonnet-20250219\")\n",
    "gemini_response_path = os.path.join(review_path, \"subset_reviews_gemini-2.5-pro-preview\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv claude.json claude-3-7-sonnet-20250219.json\n",
    "! mv gemini.json gemini-2.5-pro-preview-03-25.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
