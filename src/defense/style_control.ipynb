{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a982320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json, math, gdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1bb3a5",
   "metadata": {},
   "source": [
    "# Obtaining and Cleaning the Tournament Data\n",
    "We are hosting the initial tournament results as a JSON file on Google Drive. We use the `gdown` function to download the data. The data contains all the battels and voting results collected for ranking models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26f478f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_a': 'chatglm-6b',\n",
       " 'model_b': 'koala-13b',\n",
       " 'winner': 'model_b',\n",
       " 'judge': '2e9c29aa140b8e50643235eab01dc9ea',\n",
       " 'turn': 1,\n",
       " 'anony': True,\n",
       " 'language': 'English',\n",
       " 'tstamp': 1682351591.1322,\n",
       " 'conv_metadata': {'sum_user_tokens': 10,\n",
       "  'sum_assistant_a_tokens': 171,\n",
       "  'sum_assistant_b_tokens': 373,\n",
       "  'context_a_tokens': 10,\n",
       "  'context_b_tokens': 10,\n",
       "  'turns': 1,\n",
       "  'header_count_a': {'h1': 0, 'h2': 0, 'h3': 0, 'h4': 0, 'h5': 0, 'h6': 0},\n",
       "  'list_count_a': {'ordered': 0, 'unordered': 0},\n",
       "  'bold_count_a': {'**': 0, '__': 0},\n",
       "  'header_count_b': {'h1': 0, 'h2': 0, 'h3': 0, 'h4': 0, 'h5': 0, 'h6': 0},\n",
       "  'list_count_b': {'ordered': 5, 'unordered': 0},\n",
       "  'bold_count_b': {'**': 0, '__': 0}},\n",
       " 'is_code': True,\n",
       " 'is_refusal': False,\n",
       " 'dedup_tag': {'high_freq': False, 'sampled': True},\n",
       " 'category_tag': {'if_v0.1': {'if': False, 'score': 1},\n",
       "  'math_v0.1': {'math': False},\n",
       "  'criteria_v0.1': {'specificity': True,\n",
       "   'domain_knowledge': True,\n",
       "   'complexity': False,\n",
       "   'problem_solving': False,\n",
       "   'creativity': False,\n",
       "   'technical_accuracy': True,\n",
       "   'real_world': True}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use the latest data\n",
    "path_dir = \"/mnt/hdd1/ljiahao/xianglin/llm-as-a-judge-attack/style_control\"\n",
    "path = f\"{path_dir}/clean_battle_20240826_public.json\"\n",
    "with open(path, 'rb') as file:\n",
    "    response = json.load(file)\n",
    "response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7135f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'rb') as file:\n",
    "    battles = pd.read_json(file).sort_values(ascending=True, by=[\"tstamp\"])\n",
    "battles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "battles.conv_metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa617342",
   "metadata": {},
   "outputs": [],
   "source": [
    "battles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b7df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use anony battles only for leaderboard\n",
    "battles = battles[battles[\"anony\"] == True]\n",
    "\n",
    "# we de-duplicate top 0.1% redudant prompts\n",
    "# see https://lmsys.org/blog/2024-05-17-category-hard/#note-enhancing-quality-through-de-duplication\n",
    "print(\"Before dedup: \", len(battles))\n",
    "battles = battles[battles[\"dedup_tag\"].apply(lambda x: x.get(\"sampled\", False))]\n",
    "print(\"After dedup: \", len(battles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db09846",
   "metadata": {},
   "source": [
    "\n",
    "## [Bradley-Terry model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81d2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preety_print_model_ratings(ratings):\n",
    "    df = pd.DataFrame([\n",
    "        [n, ratings[n]] for n in ratings.keys()\n",
    "    ], columns=[\"Model\", \"Elo rating\"]).sort_values(\"Elo rating\", ascending=False).reset_index(drop=True)\n",
    "    # df[\"Elo rating\"] = (df[\"Elo rating\"] + 0.5).astype(int)\n",
    "    df.index = df.index + 1\n",
    "    return df\n",
    "\n",
    "def preety_print_two_ratings(ratings_1, ratings_2, column_names):\n",
    "    df = pd.DataFrame([\n",
    "        [n, ratings_1[n], ratings_2[n]] for n in ratings_1.keys()\n",
    "    ], columns=[\"Model\", column_names[0], column_names[1]]).sort_values(column_names[0], ascending=False).reset_index(drop=True)\n",
    "    df[column_names[0]] = (df[column_names[0]] + 0.5).astype(int)\n",
    "    df[column_names[1]] = (df[column_names[1]] + 0.5).astype(int)\n",
    "    df.index = df.index + 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bt(\n",
    "    df, SCALE=400, BASE=10, INIT_RATING=1000, sample_weight=None\n",
    "):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    ptbl_a_win = pd.pivot_table(\n",
    "        df[df[\"winner\"] == \"model_a\"],\n",
    "        index=\"model_a\",\n",
    "        columns=\"model_b\",\n",
    "        aggfunc=\"size\",\n",
    "        fill_value=0,\n",
    "    )\n",
    "    # if no tie, create a zero matrix\n",
    "    if sum(df[\"winner\"].isin([\"tie\", \"tie (bothbad)\"])) == 0:\n",
    "        ptbl_tie = pd.DataFrame(0, index=ptbl_a_win.index, columns=ptbl_a_win.columns)\n",
    "    else:\n",
    "        ptbl_tie = pd.pivot_table(\n",
    "            df[df[\"winner\"].isin([\"tie\", \"tie (bothbad)\"])],\n",
    "            index=\"model_a\",\n",
    "            columns=\"model_b\",\n",
    "            aggfunc=\"size\",\n",
    "            fill_value=0,\n",
    "        )\n",
    "        ptbl_tie = ptbl_tie + ptbl_tie.T\n",
    "\n",
    "    ptbl_b_win = pd.pivot_table(\n",
    "        df[df[\"winner\"] == \"model_b\"],\n",
    "        index=\"model_a\",\n",
    "        columns=\"model_b\",\n",
    "        aggfunc=\"size\",\n",
    "        fill_value=0,\n",
    "    )\n",
    "    ptbl_win = ptbl_a_win * 2 + ptbl_b_win.T * 2 + ptbl_tie\n",
    "\n",
    "    models = pd.Series(np.arange(len(ptbl_win.index)), index=ptbl_win.index)\n",
    "\n",
    "    p = len(models)\n",
    "    X = np.zeros([p * (p - 1) * 2, p])\n",
    "    Y = np.zeros(p * (p - 1) * 2)\n",
    "\n",
    "    cur_row = 0\n",
    "    sample_weights = []\n",
    "    for m_a in ptbl_win.index:\n",
    "        for m_b in ptbl_win.columns:\n",
    "            if m_a == m_b:\n",
    "                continue\n",
    "            # if nan skip\n",
    "            if math.isnan(ptbl_win.loc[m_a, m_b]) or math.isnan(ptbl_win.loc[m_b, m_a]):\n",
    "                continue\n",
    "            X[cur_row, models[m_a]] = +math.log(BASE)\n",
    "            X[cur_row, models[m_b]] = -math.log(BASE)\n",
    "            Y[cur_row] = 1.0\n",
    "            sample_weights.append(ptbl_win.loc[m_a, m_b])\n",
    "\n",
    "            X[cur_row + 1, models[m_a]] = math.log(BASE)\n",
    "            X[cur_row + 1, models[m_b]] = -math.log(BASE)\n",
    "            Y[cur_row + 1] = 0.0\n",
    "            sample_weights.append(ptbl_win.loc[m_b, m_a])\n",
    "            cur_row += 2\n",
    "    X = X[:cur_row]\n",
    "    Y = Y[:cur_row]\n",
    "\n",
    "    lr = LogisticRegression(fit_intercept=False, penalty=None, tol=1e-6)\n",
    "    lr.fit(X, Y, sample_weight=sample_weights)\n",
    "    elo_scores = SCALE * lr.coef_[0] + INIT_RATING\n",
    "    if \"mixtral-8x7b-instruct-v0.1\" in models.index:\n",
    "        # anchor\n",
    "        elo_scores += 1114 - elo_scores[models[\"mixtral-8x7b-instruct-v0.1\"]]\n",
    "    return pd.Series(elo_scores, index=models.index).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef43ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_ratings = compute_bt(battles)\n",
    "preety_print_model_ratings(bt_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d95bb",
   "metadata": {},
   "source": [
    "### Compute Bootstrap Confidence Interavals for BT scores\n",
    "\n",
    "We can further use bootstrap to estimate the confidence intervals as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f774ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_result(battles, func_compute_elo, num_round):\n",
    "    rows = []\n",
    "    for i in tqdm(range(num_round), desc=\"bootstrap\"):\n",
    "        rows.append(func_compute_elo(battles.sample(frac=1.0, replace=True)))\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df[df.median().sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_ROUNDS = 10 # 10 for demo purpose\n",
    "\n",
    "np.random.seed(42)\n",
    "bootstrap_elo_lu = get_bootstrap_result(battles, compute_bt, BOOTSTRAP_ROUNDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d553d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bootstrap_scores(df, title):\n",
    "    bars = pd.DataFrame(dict(\n",
    "        lower = df.quantile(.025),\n",
    "        rating = df.quantile(.5),\n",
    "        upper = df.quantile(.975))).reset_index(names=\"model\").sort_values(\"rating\", ascending=False)\n",
    "    bars['error_y'] = bars['upper'] - bars[\"rating\"]\n",
    "    bars['error_y_minus'] = bars['rating'] - bars[\"lower\"]\n",
    "    bars['rating_rounded'] = np.round(bars['rating'], 2)\n",
    "    fig = px.scatter(bars, x=\"model\", y=\"rating\", error_y=\"error_y\",\n",
    "                     error_y_minus=\"error_y_minus\", text=\"rating_rounded\",\n",
    "                     title=title)\n",
    "    fig.update_layout(xaxis_title=\"Model\", yaxis_title=\"Rating\",\n",
    "                      height=600)\n",
    "    return fig\n",
    "\n",
    "fig = visualize_bootstrap_scores(bootstrap_elo_lu, \"Bootstrap of BT Rating Estimates\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359dd8ec",
   "metadata": {},
   "source": [
    "# Style Control Arena\n",
    "Check our blog: [Controlling for Style in Chatbot Arena]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65770935",
   "metadata": {},
   "source": [
    "**Background**: We controlled for the effect of style by adding extra “style features” into our Bradley-Terry regression. This is a [standard technique](https://en.wikipedia.org/wiki/Controlling_for_a_variable) in statistics, and has been recently used in LLM evaluations [1](https://arxiv.org/abs/2404.04475). The idea is that, by including any confounding variables (e.g. response length) in the regression, we can attribute any increase in strength to the confounder, as opposed to the model. Then, the Bradley-Terry coefficient will be more reflective of the model’s intrinsic properties, as opposed to undesirable confounders. The definition of a confounder is to some extent up to our interpretation; as our style features, we use the (normalized) difference in response lengths, the number of markdown headers, and the number of lists.\n",
    "\n",
    "Formally, we define length difference as a features:\n",
    "- Token length difference between answer A and answer B\n",
    "\n",
    "$$\\text{normalize }(\\frac{\\text{length}_A - \\text{length}_B}{\\text{length}_A + \\text{length}_B})$$\n",
    "\n",
    "Similarily, we also define 3 markdown elements,\n",
    "- Markdown header elements\n",
    "- Markdown list elements\n",
    "- Markdown bold elements\n",
    "\n",
    "We normalize each features before apply logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d8b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_CONTROL_ELEMENTS_V1 = [\n",
    "    \"sum_assistant_a_tokens\",\n",
    "    \"header_count_a\",\n",
    "    \"list_count_a\",\n",
    "    \"bold_count_a\",\n",
    "    \"sum_assistant_b_tokens\",\n",
    "    \"header_count_b\",\n",
    "    \"list_count_b\",\n",
    "    \"bold_count_b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c6474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bt(X, Y, models, indices=None, SCALE=400, INIT_RATING=1000):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    p = len(models.index)\n",
    "\n",
    "    lr = LogisticRegression(fit_intercept=False)\n",
    "    if indices:\n",
    "        lr.fit(X[indices], Y[indices])\n",
    "    else:\n",
    "        lr.fit(X, Y)\n",
    "\n",
    "    elo_scores = SCALE * lr.coef_[0] + INIT_RATING\n",
    "    # calibrate llama-13b to 800 if applicable\n",
    "    if \"mixtral-8x7b-instruct-v0.1\" in models.index:\n",
    "        elo_scores += 1114 - elo_scores[models[\"mixtral-8x7b-instruct-v0.1\"]]\n",
    "    return (\n",
    "        pd.Series(elo_scores[:p], index=models.index).sort_values(ascending=False),\n",
    "        lr.coef_[0][p:],\n",
    "    )\n",
    "\n",
    "\n",
    "def construct_style_matrices(\n",
    "    df,\n",
    "    BASE=10,\n",
    "    apply_ratio=[1, 1, 1, 1],\n",
    "    style_elements=STYLE_CONTROL_ELEMENTS_V1,\n",
    "    add_one=True,\n",
    "):\n",
    "    models = pd.concat([df[\"model_a\"], df[\"model_b\"]]).unique()\n",
    "    models = pd.Series(np.arange(len(models)), index=models)\n",
    "\n",
    "    # duplicate battles\n",
    "    df = pd.concat([df, df], ignore_index=True)\n",
    "    p = len(models.index)\n",
    "    n = df.shape[0]\n",
    "    assert len(style_elements) % 2 == 0\n",
    "    k = int(len(style_elements) / 2)\n",
    "\n",
    "    X = np.zeros([n, p + k])\n",
    "    X[np.arange(n), models[df[\"model_a\"]]] = +math.log(BASE)\n",
    "    X[np.arange(n), models[df[\"model_b\"]]] = -math.log(BASE)\n",
    "\n",
    "    # creates turn each of the specified column in \"conv_metadata\" into a vector\n",
    "    style_vector = np.array(\n",
    "        [\n",
    "            df.conv_metadata.map(\n",
    "                lambda x: x[element]\n",
    "                if type(x[element]) is int\n",
    "                else sum(x[element].values())\n",
    "            ).tolist()\n",
    "            for element in style_elements\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    style_diff = (style_vector[:k] - style_vector[k:]).astype(float)\n",
    "    style_sum = (style_vector[:k] + style_vector[k:]).astype(float)\n",
    "\n",
    "    if add_one:\n",
    "        style_sum = style_sum + np.ones(style_diff.shape)\n",
    "\n",
    "    apply_ratio = np.flatnonzero(apply_ratio)\n",
    "\n",
    "    style_diff[apply_ratio] /= style_sum[\n",
    "        apply_ratio\n",
    "    ]  # Apply ratio where necessary (length, etc)\n",
    "\n",
    "    style_mean = np.mean(style_diff, axis=1)\n",
    "    style_std = np.std(style_diff, axis=1)\n",
    "\n",
    "    X[:, -k:] = ((style_diff - style_mean[:, np.newaxis]) / style_std[:, np.newaxis]).T\n",
    "\n",
    "    # one A win => two A win\n",
    "    Y = np.zeros(n)\n",
    "    Y[df[\"winner\"] == \"model_a\"] = 1.0\n",
    "\n",
    "    # one tie => one A win + one B win\n",
    "    # find tie + tie (both bad) index\n",
    "    tie_idx = (df[\"winner\"] == \"tie\") | (df[\"winner\"] == \"tie (bothbad)\")\n",
    "    tie_idx[len(tie_idx) // 2 :] = False\n",
    "    Y[tie_idx] = 1.0\n",
    "\n",
    "    return X, Y, models\n",
    "\n",
    "\n",
    "def get_bootstrap_result_style_control(X, Y, battles, models, func_compute_elo, num_round=1000):\n",
    "    elos = []\n",
    "    coefs = []\n",
    "    assert X.shape[0] % 2 == 0 and X.shape[0] == Y.shape[0]\n",
    "    k = int(\n",
    "        X.shape[0] / 2\n",
    "    )  # Since we duplicate the battles when constructing X and Y, we don't want to sample the duplicates\n",
    "\n",
    "    battles_tie_idx = (battles[\"winner\"] == \"tie\") | (battles[\"winner\"] == \"tie (bothbad)\")\n",
    "    for _ in tqdm(range(num_round), desc=\"bootstrap\"):\n",
    "        indices = np.random.choice(list(range(k)), size=(k), replace=True)\n",
    "\n",
    "        index2tie = np.zeros(k, dtype=bool)\n",
    "        index2tie[battles_tie_idx] = True\n",
    "\n",
    "        nontie_indices = indices[~index2tie[indices]]\n",
    "        tie_indices = np.concatenate([indices[index2tie[indices]], indices[index2tie[indices]]+k])\n",
    "\n",
    "        _X = np.concatenate([X[nontie_indices], X[nontie_indices], X[tie_indices]])\n",
    "        _Y = np.concatenate([Y[nontie_indices], Y[nontie_indices], Y[tie_indices]])\n",
    "\n",
    "        assert _X.shape == X.shape and _Y.shape == Y.shape\n",
    "\n",
    "        states = ~_X[:, : len(models)].any(axis=0)\n",
    "\n",
    "        elo, coef = func_compute_elo(_X, _Y, models=models[~states])\n",
    "        elos.append(elo)\n",
    "        coefs.append(coef)\n",
    "\n",
    "    df = pd.DataFrame(elos)\n",
    "    return df[df.median().sort_values(ascending=False).index], coefs\n",
    "\n",
    "\n",
    "def visualize_bootstrap_scores(df, title):\n",
    "    bars = pd.DataFrame(dict(\n",
    "        lower = df.quantile(.025),\n",
    "        rating = df.quantile(.5),\n",
    "        upper = df.quantile(.975))).reset_index(names=\"model\").sort_values(\"rating\", ascending=False)\n",
    "    bars['error_y'] = bars['upper'] - bars[\"rating\"]\n",
    "    bars['error_y_minus'] = bars['rating'] - bars[\"lower\"]\n",
    "    bars['rating_rounded'] = np.round(bars['rating'], 2)\n",
    "    fig = px.scatter(bars, x=\"model\", y=\"rating\", error_y=\"error_y\",\n",
    "                     error_y_minus=\"error_y_minus\", text=\"rating_rounded\",\n",
    "                     title=title)\n",
    "    fig.update_layout(xaxis_title=\"Model\", yaxis_title=\"Rating\",\n",
    "                      height=600)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b00a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_battles = battles[-200000:] # for demo purpose\n",
    "len(recent_battles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb0e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, models = construct_style_matrices(recent_battles)\n",
    "elo_rating_style, style_coef = fit_bt(X, Y, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BT Coefficients for the style elements\")\n",
    "print(f\"Length: {round(style_coef[0], 3)}, Markdown Header: {round(style_coef[1], 3)}, Markdown List: {round(style_coef[2], 3)}, Markdown Bold: {round(style_coef[3], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec6cda",
   "metadata": {},
   "source": [
    "The BT coefficient for each style elements represent how much impact does it has on the win-rate probability. We can see that length has the strongest affect on win-rate. Now let's check the new ranking! Notice it is quite different from the vanilla leaderboard without controlling style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c94214",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elo_rating_style[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa66b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_ROUNDS = 100\n",
    "\n",
    "bootstrap_df, boostrap_coef = get_bootstrap_result_style_control(\n",
    "    X, Y, recent_battles, models, fit_bt, num_round=BOOTSTRAP_ROUNDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35424c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_bootstrap_scores(bootstrap_df, f\"Bootstrap of BT score Style Controlled\")\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
